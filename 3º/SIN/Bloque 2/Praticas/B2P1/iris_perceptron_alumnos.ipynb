{"cells":[{"cell_type":"markdown","metadata":{"id":"fXgAhyAdgyvS"},"source":["# Perceptrón aplicado a iris"]},{"cell_type":"markdown","metadata":{"id":"qMuI1k_AgyvT"},"source":["**Lectura del corpus:** $\\;$ comprobamos también que las matrices de datos y etiquetas tienen las filas y columnas que toca"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KqKpXRcFgyvU"},"outputs":[],"source":["import numpy as np; from sklearn.datasets import load_iris\n","iris = load_iris(); X = iris.data.astype(np.float16);\n","y = iris.target.astype(np.uint).reshape(-1, 1);\n","print(X.shape, y.shape, \"\\n\", np.hstack([X, y])[:5, :])"]},{"cell_type":"markdown","metadata":{"id":"bNj7PTU5gyvU"},"source":["**Partición del corpus:** $\\;$ Creamos un split de iris con un $20\\%$ de datos para test y el resto para entrenamiento (training), barajando previamente los datos de acuerdo con una semilla dada para la generación de números aleatorios. Aquí, como en todo código que incluya aleatoriedad (que requiera generar números aleatorios), conviene fijar dicha semilla para poder reproducir experimentos con exactitud."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d3JVbmXUgyvV"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=23)\n","print(X_train.shape, X_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"Qy73P_ssgyvV"},"source":["**Implementación de Perceptrón:** $\\;$ devuelve pesos en notación homogénea, $\\mathbf{W}\\in\\mathbb{R}^{(1+D)\\times C};\\;$ también el número de errores e iteraciones ejecutadas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WIzbuFofgyvV"},"outputs":[],"source":["def perceptron(X, y, b=0.1, a=1.0, K=200):\n","    N, D = X.shape; Y = np.unique(y); C = Y.size; W = np.zeros((1+D, C))\n","    for k in range(1, K+1):\n","        E = 0\n","        for n in range(N):\n","            xn = np.array([1, *X[n, :]])\n","            cn = np.squeeze(np.where(Y==y[n]))\n","            gn = W[:,cn].T @ xn; err = False\n","            for c in np.arange(C):\n","                if c != cn and W[:,c].T @ xn + b >= gn:\n","                    W[:, c] = W[:, c] - a*xn; err = True\n","            if err:\n","                W[:, cn] = W[:, cn] + a*xn; E = E + 1\n","        if E == 0:\n","            break;\n","    return W, E, k"]},{"cell_type":"markdown","metadata":{"id":"y31EDfj7gyvW"},"source":["**Aprendizaje de un clasificador (lineal) con Perceptrón:** $\\;$ Perceptrón minimiza el número de errores de entrenamiento (con margen)\n","$$\\mathbf{W}^*=\\operatorname*{argmin}_{\\mathbf{W}=(\\boldsymbol{w}_1,\\dotsc,\\boldsymbol{w}_C)}\\sum_n\\;\\mathbb{Y}\\biggl(\\max_{c\\neq y_n}\\;\\boldsymbol{w}_c^t\\boldsymbol{x}_n+b \\;>\\; \\boldsymbol{w}_{y_n}^t\\boldsymbol{x}_n\\biggr)$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iAuEF10bgyvX"},"outputs":[],"source":["W, E, k = perceptron(X_train, y_train)\n","print(\"Número de iteraciones ejecutadas: \", k)\n","print(\"Número de errores de entrenamiento: \", E)\n","print(\"Vectores de pesos de las clases (en columnas y en notación homogénea):\\n\", W);"]},{"cell_type":"markdown","metadata":{"id":"u7G-a5_zgyvX"},"source":["**Cálculo de la tasa de error en test:**\n","El siguiente conjunto de instrucciones calcula la tasa de error para un Perceptron entrenado con b=0.1 y a=1.0 (factor de aprendizaje o 'a') y K=200 iteraciones"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1W2qNWQggyvY"},"outputs":[],"source":["X_testh = np.hstack([np.ones((len(X_test), 1)), X_test])\n","y_test_pred  = np.argmax(X_testh @ W, axis=1).reshape(-1, 1)\n","err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n","print(f\"Tasa de error en test: {err_test:.1%}\")"]},{"cell_type":"markdown","metadata":{"id":"43TAexM_gyvY"},"source":["**Ajuste del margen:** $\\;$ el siguiente bucle es un experimento que ejecuta 5 veces el algoritmo del Perceptron con 5 valores diferentes de $b$ y valor por defecto $α$=1.0 y K=1000 iteraciones, mostrando el error de entrenamiento para cada valor de $b$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5EokibkXgyvY"},"outputs":[],"source":["for b in (.0, .01, .1, 10, 100):\n","    W, E, k = perceptron(X_train, y_train, b=b, K=1000)\n","    print(b, E, k)"]},{"cell_type":"markdown","metadata":{"id":"faP8KC55gyvY"},"source":["**Interpretación de resultados:** $\\;$ los datos de entrenamiento no parecen linealmente separables; no está claro que un margen mayor que cero pueda mejorar resultados, sobre todo porque solo tenemos $30$ muestras de test; con margen nulo ya hemos visto que se obtiene un error (en test) del $16.7\\%$\n","-"]},{"cell_type":"markdown","source":["**Ejercicio para realizar en clase:**\n","---\n","\n","\n","Escribe el código python necesario para mostrar resultados del error de entrenamiento (E), número de iteraciones empleadas (k) y error de test (err_test) para combinaciones de diferentes valores de:\n","---\n","\n","# *   a: (factor de aprendizaje: utiliza valores 0.1, 1.0, 10, 100, 1000, 10000)\n","# *   b: (margen: utiliza valores 0.0, 0.01, 0.1, 1.0, 100, 1000)\n","# *   K: (iteraciones: utiliza valores 200, 500, 800 y 1000)\n","\n","\n","Utiliza la siguientes instrucciones para mostrar la cabecera:\n","---\n","\n","`print('#      a        b      K      E      k      Ete');`\n","---\n","`print('#-------   ------   ----   ----   ----     ----');`\n","---\n","\n","y la siguiente instrucción para mostrar los resultados:\n","\n","`print('%8.2f %8.2f %6d %6d %6d %8.3f' %(a, b, K, E,k,err_test))`\n","---\n","\n","\n","\n"],"metadata":{"id":"5wMTWdG3xWUH"}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"orig_nbformat":4,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}